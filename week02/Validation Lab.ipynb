{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1cde85-9d6a-4177-a256-3fb5e5d255de",
   "metadata": {},
   "source": [
    "# Cross Validation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2eb71-b527-4a5f-97bc-5af21465aa2f",
   "metadata": {},
   "source": [
    "In this notebook, I am going to use the mtcars dataset as a worked example. You are to read this, try running the cells and understand what is going on in them. Get the logic behind the things I've tried\n",
    "\n",
    "1) Looking at train_test_split and comparing training scores to test scores.\n",
    "2) Getting cross validation scores to make a selection on which features to use.\n",
    "3) Trying Polynomial Regression on a variable to see if it can improve things.\n",
    "\n",
    "After this, I want you to repeat the procedure with the Diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f988b-b06d-49c6-aa51-4184409f2fff",
   "metadata": {},
   "source": [
    "First of all, let's look at the mtcars (1974 Motor Trend Cars) dataset and see what we can do with that.\n",
    "<br><img src=\"images/mtcars.png\" width=\"500\"/><br>\n",
    "Let's try more than one feature and see if this improves the model. We should be using separate sets for training and evaluating so let's try that now and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a23e1c-8fc0-4d78-8904-d19967beb4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb4751-90f1-4950-ac3d-0a7d75a17fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mtcars\n",
    "dfcars=pd.read_csv(\"week02\\mtcars.csv\")\n",
    "dfcars=dfcars.rename(columns={\"Unnamed: 0\":\"name\"})\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6e45b-4e86-42ce-9331-ffe4ed77475e",
   "metadata": {},
   "source": [
    "y is the response variable so let's slice that off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6afa67-1539-4f7d-9a89-3dd9a966d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfcars['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23991b-e1a7-470d-ae97-a0e11e03d0cd",
   "metadata": {},
   "source": [
    "We have multiple possible X's let's put them aside too, we don't want the first two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589c4746-bde3-4ec7-a35f-6d2b1745c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "allX = dfcars.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b78aa7-11a5-4352-9425-ba06158ad350",
   "metadata": {},
   "outputs": [],
   "source": [
    "allX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d5f9a-e3d0-48cc-879d-41b4a70d1b84",
   "metadata": {},
   "source": [
    "We first of all need to separate the data into a training set and a test set. We can use train_test_split for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b897923-eeb8-4b39-99b2-84177201b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d91301-90d9-4d6b-89dd-65170aa02d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "allX_train, allX_test, y_train, y_test = train_test_split(allX, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a7d11-6a58-4f9a-9142-549dc9e7aa7f",
   "metadata": {},
   "source": [
    "I did random_state=42 so the answer is the same everytime I run this workbook.\n",
    "\n",
    "Ok now we have two different datasets, train and test that we can use where appropriate. First of all, we just want to try different features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b74d02-694f-407c-bb10-1fe60d57155a",
   "metadata": {},
   "source": [
    "I think wt would be a good feature to select so I'll go with that\n",
    "\n",
    "Separating out just that column into what we are going to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b723999-e5cf-4f4c-a012-77ed15d3ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\"]]\n",
    "X_test = allX_test[[\"wt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61877c4f-8f5d-4f9f-98d3-3c9682be8af3",
   "metadata": {},
   "source": [
    "Now build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24025ee-df55-43c2-85a1-91e9d86aadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bac103-ca81-43bb-8411-6ed5d05d50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf743f8-b787-499d-82bf-cea861c2c793",
   "metadata": {},
   "source": [
    "Let's look at the scores of both the training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d75d0-25e4-400c-a082-39aec17eab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ded12f-159d-4ee8-be2e-aa66ef3e472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15066591-6cc5-4f21-83b9-ff8e892698f9",
   "metadata": {},
   "source": [
    "There is a decent difference between the scores so maybe overfit a little. Anyway let's try a different feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "452bc5c6-a43e-47f0-bb3f-dc4b7cb11123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"hp\"]]\n",
    "X_test = allX_test[[\"hp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b1971-f0d6-403a-b1a0-d4163d5252cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbd9bd-fc20-4494-9af4-5627b9d5eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c27d4f-c346-4311-8cc1-a5fd1d13c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1b0fc-d69b-402e-b3eb-6d2819067339",
   "metadata": {},
   "source": [
    "Ok hp is even worse performing than wt. Not a good one to use\n",
    "\n",
    "drat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "884519ab-307b-451f-9f7e-76ce5d375f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"drat\"]]\n",
    "X_test = allX_test[[\"drat\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b0d6f-6aed-4659-8d40-98d142ef12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LinearRegression()\n",
    "model3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d4c3b-48ba-468b-a2f8-99ac3aca48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4d089-986d-417a-8153-b4e58db746ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3e1d2-1dd0-4b7d-84d9-d91038ffcf96",
   "metadata": {},
   "source": [
    "The best I tried was wt, the first one. Anyway let's try two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488982db-c5c8-4c7c-a0a5-72d0f8aead83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\",\"hp\"]]\n",
    "X_test = allX_test[[\"wt\",\"hp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d429d-e668-47f6-ac96-ec7b5aa64c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LinearRegression()\n",
    "model4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fb7f5-46f1-4d9b-adec-afa2619e49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba462ae-4ed1-4f52-95d6-22f443da81ef",
   "metadata": {},
   "source": [
    "That is a better training score, but what about the test score which is really what's important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1295bd-b965-4dd8-8c62-b0d5e2c47ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bdd6b-6f55-4dce-a0e9-f6b28f2c11c5",
   "metadata": {},
   "source": [
    "That's the best we've had so far, so model4 is the best one of that\n",
    "\n",
    "## Validation\n",
    "Hold on, I've contradicted a little what I said in the \"lecture\". I'm using the test score to make decisions. I should not be doing this. I should use some form of validation. So here's an example of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f888140-ee3e-4c10-9ff0-68284df95dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39defd5-643c-46f4-80ac-892fb1401139",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba9c96-5ed8-4f98-9084-2e14e26e8b4e",
   "metadata": {},
   "source": [
    "Let's try wt again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad41e937-066b-4f26-b92e-69d1ad30f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\"]]\n",
    "X_test = allX_test[[\"wt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce480d-dca2-479e-a62e-fd0a472da4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = LinearRegression()\n",
    "scores = cross_val_score(model5, X_train, y_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bc75b-78f9-4f66-b9cf-004d4c0b7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18127986-2289-4f19-b4ab-a149c4b04920",
   "metadata": {},
   "source": [
    "Now let's try the two variables, wt and hp again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0abcd2cc-2677-4fa4-b9a6-06e3582c6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\",\"hp\"]]\n",
    "X_test = allX_test[[\"wt\",\"hp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c67d4-2e2b-4d97-ad6d-bde8de1f1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = LinearRegression()\n",
    "scores6 = cross_val_score(model5, X_train, y_train)\n",
    "scores6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc2835-fdab-4cd4-b054-67393e781962",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores6.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633714ee-12b2-4947-94cd-8fae63bcdd86",
   "metadata": {},
   "source": [
    "So the two together has a higher cross validation score that just one. This means we should select the two variables over one \n",
    "\n",
    "What about 3? Let's try drat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82bb545a-b8d1-4806-8855-00a0efe900fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\",\"hp\",\"drat\"]]\n",
    "X_test = allX_test[[\"wt\",\"hp\",\"drat\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f26bf-1519-4784-889c-09c986dece99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model7 = LinearRegression()\n",
    "scores7 = cross_val_score(model5, X_train, y_train)\n",
    "scores7.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa71308-1e0a-4b00-ad62-3d7fb48bd617",
   "metadata": {},
   "source": [
    "Slightly better than 2. It is not always the case that more features the better, just happened in the above example\n",
    "\n",
    "Let's try three features again, but this time with carb instead of drat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df32aca6-719d-4793-a931-e2f0791045fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\",\"hp\",\"carb\"]]\n",
    "X_test = allX_test[[\"wt\",\"hp\",\"carb\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28df02-285f-40f3-ba40-e7ac33fda2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = LinearRegression()\n",
    "scores8 = cross_val_score(model5, X_train, y_train)\n",
    "scores8.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40495f4-fd76-4066-88b0-ba35a97c223b",
   "metadata": {},
   "source": [
    "This score is worse than only using the two features wt and hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072e719-36b9-4f8c-9f21-3f13f7e36a81",
   "metadata": {},
   "source": [
    "Our best so far (according to the cross val scores) has been model7, the one with [\"wt\",\"hp\",\"drat\"]. So let's build that model fully and then evaluate that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc49b90-72a2-4c47-a9b0-84d7a758a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\",\"hp\",\"drat\"]]\n",
    "X_test = allX_test[[\"wt\",\"hp\",\"drat\"]]\n",
    "model7 = LinearRegression()\n",
    "model7.fit(X_train, y_train)\n",
    "model7.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636c970-735a-4f5e-ba85-74e863106bce",
   "metadata": {},
   "source": [
    "A score of .79 for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72cf9f7-cec1-4f63-a43e-7ff1ec33e857",
   "metadata": {},
   "source": [
    "## Polynomial Regression Example\n",
    "\n",
    "I am going to use the 'wt' as a feature and see what happens with a polynomial regression model\n",
    "\n",
    "Some notes about Polynomial and sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b6710-be52-4e66-9219-d91ce9df6674",
   "metadata": {},
   "source": [
    "### The `scikit-learn` interface\n",
    "\n",
    "Scikit-learn is the main python machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split`. It can be used in python by the incantation `import sklearn`.\n",
    "\n",
    "The library has a very well defined interface. This makes the library a joy to use, and surely contributes to its popularity. As the [scikit-learn API paper](http://arxiv.org/pdf/1309.0238v1.pdf) [Buitinck, Lars, et al. \"API design for machine learning software: experiences from the scikit-learn project.\" arXiv preprint arXiv:1309.0238 (2013).] says:\n",
    "\n",
    ">All objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: **an estimator interface for building and ﬁtting models, a predictor interface for making predictions and a transformer interface for converting data**. The estimator interface is at the core of the library. It deﬁnes instantiation mechanisms of objects and exposes a `fit` method for learning a model from training data. All supervised and unsupervised learning algorithms (e.g., for classiﬁcation, regression or clustering) are oﬀered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n",
    "\n",
    "We'll use the \"estimator\" interface here, specifically the estimator `PolynomialFeatures`. The API paper again:\n",
    "\n",
    ">Since it is common to modify or ﬁlter data before feeding it to a learning algorithm, some estimators in the library implement a transformer interface which deﬁnes a transform method. It takes as input some new data X and yields as output a transformed version of X. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n",
    "\n",
    "To start with we have one **feature** `x` to predict `y`, what we will do is the transformation:\n",
    "\n",
    "$$ x \\rightarrow 1, x, x^2, x^3, ..., x^d $$\n",
    "\n",
    "for some power $d$. Our job then is to **fit** for the coefficients of these features in the polynomial\n",
    "\n",
    "$$ a_0 + a_1 x + a_2 x^2 + ... + a_d x^d. $$\n",
    "\n",
    "In other words, we have transformed a function of one feature, into a (rather simple) **linear** function of many features. To do this we first construct the estimator as `PolynomialFeatures(d)`, and then transform these features into a d-dimensional space using the method `fit_transform`.\n",
    "\n",
    "![fit_transform](images/sklearntrans.jpg)\n",
    "\n",
    "Here is an example. The reason for using `[[1],[2],[3]]` as opposed to `[1,2,3]` is that scikit-learn expects data to be stored in a two-dimensional array or matrix with size `[n_samples, n_features]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095c55e-f5be-427f-a6bd-508262111b3c",
   "metadata": {},
   "source": [
    "To transform `[1,2,3]` into [[1],[2],[3]] we need to do a reshape.\n",
    "\n",
    "![reshape](images/reshape.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fc670-47ea-4ceb-aa50-7471b5cc1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = np.array([1,2,3]).reshape(-1,1)\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec976076-85e2-4f27-8cb0-c22fbce946d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2214d5-9648-42fd-affa-d03de34e8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolynomialFeatures(5).fit_transform(demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47f66d-690b-4807-a4ce-14c8b61bcdf2",
   "metadata": {},
   "source": [
    "This is $x^0 , x^1, x^2, x^3, x^4, x^5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64e18f9c-c1cf-4549-8b9b-0dce5ce960c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allX_train[[\"wt\"]]\n",
    "X_test = allX_test[[\"wt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d840bd3-6344-4bcc-b82b-603c513c16f8",
   "metadata": {},
   "source": [
    "Let's try it with PolynomialFeatures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4f1d428-7adc-43d9-a41c-65c098d73504",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = PolynomialFeatures(2).fit_transform(X_train)\n",
    "X_test_poly = PolynomialFeatures(2).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b45e5-b50b-4faa-9121-a226f766f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpoly2 = LinearRegression()\n",
    "modelpoly2.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ce173-a44e-4f0a-aa0a-d27bb168c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelpoly2.score(X_train_poly, y_train))\n",
    "print(modelpoly2.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0712b-3acb-420f-9555-1b9210263a34",
   "metadata": {},
   "source": [
    "Let's now do a loop for more of them and record all the scores in an array\n",
    "\n",
    "We will initialise the array as all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b973a25f-54fd-45e7-a5a0-519c1ef9b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6654b70b-ef0a-4dc8-ae58-1887e242b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = 10\n",
    "degrees = range(max_p+1)\n",
    "train_scores = np.zeros(max_p+1)\n",
    "test_scores = np.zeros(max_p+1)\n",
    "error_train = np.zeros(max_p+1)\n",
    "error_test = np.zeros(max_p+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e3b5f61-78ed-42c9-a3aa-0978a21489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in degrees:\n",
    "    if i != 0:\n",
    "        X_train_poly = PolynomialFeatures(i).fit_transform(X_train)\n",
    "        X_test_poly = PolynomialFeatures(i).fit_transform(X_test)\n",
    "        polymodel = LinearRegression()\n",
    "        polymodel.fit(X_train_poly, y_train)\n",
    "        prediction_on_training = polymodel.predict(X_train_poly)\n",
    "        prediction_on_test = polymodel.predict(X_test_poly)\n",
    "        error_train[i] = mean_squared_error(y_train, prediction_on_training)\n",
    "        error_test[i] = mean_squared_error(y_test, prediction_on_test)\n",
    "        train_scores[i] = polymodel.score(X_train_poly, y_train)\n",
    "        test_scores[i] = polymodel.score(X_test_poly, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc84f8d-f703-48dd-8e8c-250449fd8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d7c79-783b-4509-91d7-c6d3cf871d29",
   "metadata": {},
   "source": [
    "Now let's look at plots and things. I am going to keep doing error_train[1:], error_test[1:] so the 0 value is not counted\n",
    "\n",
    "This will give us the degree with the lowest error in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ebedb09-5d90-4367-a837-3123e6c05f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestd = np.argmin(error_test[1:])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d18b54-7169-4c19-b756-9a9cf2611048",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees[1:], error_train[1:], marker='o', label='train (in-sample)')\n",
    "plt.plot(degrees[1:], error_test[1:], marker='o', label='test')\n",
    "plt.axvline(bestd, 0,0.5, color='g', label=\"min test error at d=%d\"%bestd, alpha=0.3)\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper left')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69b961-f253-451e-9a0b-cd2f15b54224",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "So the simple linear regression $y = w_0 + w_1x$ works best for wt\n",
    "\n",
    "Notice how the training error keeps getting smaller as the degree increases, but this does not correspond to a better test error\n",
    "\n",
    "This is expected if we looked at the scatter plot of wt vs mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54990a23-4370-40df-aa33-5785c7a20225",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dfcars[\"wt\"],dfcars[\"mpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def6db9-516d-4eb2-9b80-c91719a8c7c7",
   "metadata": {},
   "source": [
    "Relationship looks linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aabc03-39e6-4e97-8234-ee4355be34a6",
   "metadata": {},
   "source": [
    "## Some work for you to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb5234",
   "metadata": {},
   "source": [
    "## Firstly - Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8865b3",
   "metadata": {},
   "source": [
    "What we have done in picking a given degree $d$ as the best hypothesis is that we have used the test set as a training set. \n",
    "If we choose the best $d$ based on minimizing the test set error, we have then \"fit for\" hyperparameter $d$ on the test set. \n",
    "\n",
    "In this case, the test-set error will underestimate the true out of sample error. Furthermore, we have **contaminated the test set** by fitting for $d$ on it; it is no longer a true test set.\n",
    "\n",
    "Thus, we introduce a new **validation set** on which the complexity parameter $d$ is fit, and leave out a test set which we can use to estimate the true out-of-sample performance of our learner. The place of this set in the scheme of things is shown below:\n",
    "\n",
    "![m:caption](images/train-validate-test.png)\n",
    "\n",
    "We have split the old training set into a **new smaller training set** and a **validation set**, holding the old test aside for FINAL testing AFTER we have \"fit\" for complexity $d$. Obviously we have decreased the size of the data available for training further, but this is a price we must pay for obtaining a good estimate of the out-of-sample error \n",
    "\n",
    "![m:caption](images/train-validate-test-cont.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377ee79",
   "metadata": {},
   "source": [
    "The validation process is illustrated in these two figures. We first loop over the complexity parameter $d$, the degree of the polynomials we will try and fit. Then for each degree $d$, we obtain a best fit model $g^-_d$ where the \"minus\" superscript indicates that we fit our model on the new training set which is obtained by removing (\"minusing\") a validation chunk (often the same size as the test chunk) from the old training set. We then \"test\" this model on the validation chunk, obtaining the validation error for the best-fit polynomial coefficients and for degree $d$. We move on to the next degree $d$ and repeat the process, just like before. We compare all the validation set errors, just like we did with the test errors earlier, and pick the degree $d_*$ which minimizes this validation set error.\n",
    "\n",
    "![caption](images/train-validate-test3.png)\n",
    "\n",
    "Having picked the hyperparameter $d_*$, we retrain on the entire old training-set to find the parameters of the polynomial of order $d_*$.  We now compute the test error on the test set as an estimate of the test error.\n",
    "\n",
    "Thus the **validation** set is the set on which the hyperparameter is fit. This method of splitting the data $\\cal{D}$ is called the **train-validate-test** split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f8c7c",
   "metadata": {},
   "source": [
    "### Fit on training and predict on validation\n",
    "\n",
    "\n",
    "We carry out this process for one training/validation split below. Note the smaller size of the new training set. We hold the test set at the same size.\n",
    "\n",
    "Firstly, let's split the training set up further into X_v_train, X_v_valid, y_v_train and y_v_valid using train_test_split again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42f8a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X_v_train, X_v_valid, y_v_train, y_v_valid = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a9f89",
   "metadata": {},
   "source": [
    "\n",
    ">YOUR TURN HERE: Train on the smaller training set. Fit for d on the validation set.  Store the respective MSEs in `error_train` and `error_valid`. Then retrain on the entire training set using this d. Label the test set MSE with the variable `err`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c658050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note code below hasn't been tested but is here for psuedocode suggestion\n",
    "\n",
    "error_train=np.empty(len(degrees))\n",
    "error_valid=np.empty(len(degrees))\n",
    "score_train=np.empty(len(degrees))\n",
    "score_valid=np.empty(len(degrees))\n",
    "#for each degree, we now fit on the smaller training set and predict on the validation set\n",
    "#we accumulate the MSE on both sets in error_train and error_valid\n",
    "#we then find the degree of polynomial that minimizes the MSE on the validation set.\n",
    "#your code here\n",
    "for d in degrees:#for increasing polynomial degrees 0,1,2...\n",
    "    #Create polynomials from X_v_train and X_v_valid\n",
    "    X_c = PolynomialFeatures(d).fit_transform(X_v_train)\n",
    "    X_c_val = PolynomialFeatures(d).fit_transform(X_v_valid)\n",
    "    #fit a model linear in polynomial coefficients on the new smaller training set\n",
    "    est = LinearRegression()\n",
    "    est.fit(X_c, y_v_train)    \n",
    "    #predict on new training and validation sets and calculate mean squared error\n",
    "    error_train[d] = mean_squared_error(est.predict(X_c), y_v_train)\n",
    "    error_valid[d] = mean_squared_error(est.predict(X_c_val), y_v_valid)\n",
    "    score_train[d] = est.score(X_c, y_v_train)\n",
    "    score_valid[d] = est.score(X_c_val, y_v_valid)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b6663",
   "metadata": {},
   "source": [
    "Plot the training error and validation error against the degree of the polynomial, and show the test set error at the $d$ which minimizes the validation set error.\n",
    "\n",
    "Fit on WHOLE training set now. You will need to remake polynomial features on the whole training set. Test on TestSet\n",
    "\n",
    "Try with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac129b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273be7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c223854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29cac39",
   "metadata": {},
   "source": [
    "# Try now with the Diabetes Dataset (more samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa0349-76ca-4538-a064-7ee6c687216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d73ff-1f2b-47db-9d3f-2d75186f1c08",
   "metadata": {},
   "source": [
    "## Do similar to what I've done with the diabetes dataset\n",
    "\n",
    "See if you can find a better combination of columns rather than using all of them\n",
    "\n",
    "Pick one variable and see if a Polynomial Regression model could improve that one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8cace4-9ec2-42ed-a77b-389fdf9c51ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
